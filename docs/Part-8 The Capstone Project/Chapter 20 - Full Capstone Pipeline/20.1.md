# 20.1: Voice Input

## The Starting Point: Human Command

In a full capstone pipeline for an autonomous humanoid, the interaction often begins with a natural language voice command from a human user. This is the robot's primary interface for receiving high-level instructions about a task it needs to perform.

## Component: Speech Recognition (Whisper)

As discussed in Chapter 13, the first crucial step is to convert the spoken audio into text. OpenAI's Whisper model (or a similar robust speech recognition system) plays a vital role here.

*   **Input:** Raw audio stream from the robot's microphones.
*   **Process:** Whisper processes the audio, filtering out noise and accurately transcribing the spoken words into a text string.
*   **Output:** A clean, textual representation of the human command (e.g., "Robot, please go to the kitchen and get me a glass of water from the fridge").

## Considerations for Voice Input

*   **Microphone Array:** Humanoid robots are typically equipped with multiple microphones to enable sound localization and improve speech recognition accuracy in noisy environments.
*   **Real-time Processing:** The speech recognition component must operate in real-time to provide immediate feedback and avoid delays in the interaction.
*   **Wake Word Detection:** For continuous listening without constant processing, a wake word (e.g., "Robot") can be used to activate the speech recognition system, reducing computational load.

The textual output from the voice input stage forms the basis for the robot's understanding and subsequent planning, initiating the autonomous execution of the human's request.
