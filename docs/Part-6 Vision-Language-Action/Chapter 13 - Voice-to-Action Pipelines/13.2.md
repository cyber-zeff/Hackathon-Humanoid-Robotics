# 13.2: Mapping Voice -> Actions

## From Text to Robot Commands

Once Whisper has transcribed spoken commands into text, the next critical step in a voice-to-action pipeline is to convert this natural language into concrete, executable actions for the robot. This involves several stages of natural language processing and understanding.

## Components of the Mapping Process

1.  **Intent Recognition:** The first step is to determine the user's intent from the transcribed text. This involves identifying the core action the user wants the robot to perform (e.g., "pick up," "move to," "wave").
2.  **Entity Extraction:** Once the intent is known, relevant entities (objects, locations, parameters) need to be extracted from the text. For example, in "pick up the red block," "red block" is the entity.
3.  **Command Generation:** The recognized intent and extracted entities are then used to generate a structured command that the robot's control system can understand. This structured command might be a ROS service call, an action goal, or a set of joint commands.

## Example: "Pick up the red block"

Let's break down the process for the command "Robot, pick up the red block":

*   **Whisper:** Converts audio to "Robot, pick up the red block."
*   **Intent Recognition:** Identifies the intent as `PICK_UP`.
*   **Entity Extraction:** Identifies `red block` as the target object.
*   **Command Generation (Conceptual):**
    ```python
    action_client.send_goal('pick_object', object_name='red block', object_color='red')
    ```
    This conceptual command would then be processed by the robot's manipulation system.

## Tools and Techniques

*   **Rule-based systems:** For simpler command sets, rules can be defined to map keywords and phrases to actions.
*   **Natural Language Understanding (NLU) models:** For more complex and flexible interaction, machine learning models (often based on deep learning) can be trained to recognize intents and extract entities. Frameworks like Rasa, spaCy, or custom-trained LLMs can be used here.
*   **Ontologies and Knowledge Graphs:** A robot's understanding of its environment and capabilities can be enriched by integrating knowledge graphs. For example, if the robot knows what a "red block" is and where it might be, it can better execute the command.
